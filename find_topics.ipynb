{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tmkin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize text\n",
    "    text = str(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stopwords, and lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tmkin\\AppData\\Local\\Temp\\ipykernel_14520\\3701194148.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  worldnews = pd.read_csv('comments_filtered/worldnews_comments.csv')\n"
     ]
    }
   ],
   "source": [
    "worldnews = pd.read_csv('comments_filtered/worldnews_comments.csv')\n",
    "\n",
    "# Extract the specific part of the URL from the 'link' column\n",
    "pattern = r'/r/worldnews/comments/\\w+/([^/]+/\\w+)'\n",
    "\n",
    "worldnews['og_post'] = worldnews['link'].str.extract(pattern, expand=False)\n",
    "worldnews['og_post'] = worldnews['og_post'].str.replace('_', ' ')\n",
    "\n",
    "# Remove rows where no match was found\n",
    "worldnews = worldnews.dropna(subset=['og_post'])\n",
    "\n",
    "worldnews['full_text'] = worldnews['og_post'] + ' ' + worldnews['body']\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "worldnews['full_text'] = worldnews['full_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary from the processed text\n",
    "dictionary = Dictionary(worldnews['full_text'])\n",
    "\n",
    "# Convert the text to a bag-of-words format (word counts)\n",
    "corpus = [dictionary.doc2bow(text) for text in worldnews['full_text']]\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.087*\"russian\" + 0.084*\"live\" + 0.069*\"thread\" + 0.067*\"invasion\" + 0.065*\"rworldnews\"')\n",
      "(1, '0.100*\"x\" + 0.069*\"refuse\" + 0.067*\"woman\" + 0.034*\"survivor\" + 0.034*\"auschwitz\"')\n",
      "(2, '0.123*\"removed\" + 0.064*\"israel\" + 0.047*\"hamas\" + 0.034*\"jew\" + 0.029*\"gaza\"')\n",
      "(3, '0.029*\"attack\" + 0.017*\"u\" + 0.016*\"air\" + 0.015*\"missile\" + 0.015*\"mongolia\"')\n",
      "(4, '0.077*\"brazil\" + 0.057*\"ban\" + 0.039*\"japan\" + 0.031*\"china\" + 0.017*\"alone\"')\n",
      "(5, '0.022*\"\\'s\" + 0.020*\"n\\'t\" + 0.018*\"ukraine\" + 0.017*\"russia\" + 0.011*\"would\"')\n",
      "(6, '0.075*\"holocaust\" + 0.031*\"france\" + 0.030*\"germany\" + 0.019*\"eu\" + 0.015*\"state\"')\n",
      "(7, '0.141*\"musk\" + 0.063*\"korean\" + 0.061*\"memorial\" + 0.059*\"defaced\" + 0.039*\"korea\"')\n",
      "(8, '0.070*\"berlin\" + 0.068*\"http\" + 0.034*\"president\" + 0.024*\"reddit\" + 0.023*\"vladimir\"')\n",
      "(9, '0.086*\"’\" + 0.014*\"people\" + 0.012*\"“\" + 0.012*\"”\" + 0.011*\"right\"')\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Train the LDA model with 50 topics\n",
    "lda_model = LdaModel(\n",
    "    corpus,\n",
    "    num_topics=50,\n",
    "    id2word=dictionary,\n",
    "    passes=10,\n",
    ")\n",
    "\n",
    "# Print the top 10 words for each topic\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write topics to txt file\n",
    "import re\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "with open('topics.txt', 'w') as f:\n",
    "    for topic_id, topic in topics:\n",
    "        words = [re.sub(r'\\d+\\.\\d+\\*?', '', word).strip() for word in re.split(r'[+*]', topic) if word.strip()]\n",
    "        f.write(f\"{topic_id}, {', '.join(words)}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
